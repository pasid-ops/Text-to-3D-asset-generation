# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dMBstD6uq6dAVRSGBopGM_kMKYR5Tlb8
"""

!pip install -q torch torchvision torchaudio

!pip install -q diffusers transformers accelerate

# Try installing libraries separately to avoid conflicts
!pip install -q opencv-python matplotlib
!pip install -q open3d
!pip install -q pythreejs trimesh

import torch
from diffusers import StableDiffusionPipeline
import cv2
import numpy as np
import open3d as o3d
import trimesh
import os
from PIL import Image
import matplotlib.pyplot as plt
from IPython.display import display
from pythreejs import *

def generate_image(prompt="a medieval fantasy helmet with horns"):
    pipe = StableDiffusionPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
    )
    pipe = pipe.to("cuda" if torch.cuda.is_available() else "cpu")

    image = pipe(prompt).images[0]
    image.save("generated.png")
    print("[âœ“] Image saved as generated.png")
    return "generated.png"

def generate_depth(image_path):
    model_type = "DPT_Hybrid"
    midas = torch.hub.load("intel-isl/MiDaS", model_type)
    midas = midas.to("cuda" if torch.cuda.is_available() else "cpu").eval()

    midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")
    transform = midas_transforms.dpt_transform

    img = cv2.imread(image_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    input_batch = transform(img_rgb).to("cuda" if torch.cuda.is_available() else "cpu")

    with torch.no_grad():
        prediction = midas(input_batch)
        depth = prediction.squeeze().cpu().numpy()

    depth = cv2.normalize(depth, None, 0, 255, cv2.NORM_MINMAX)
    depth_img = depth.astype(np.uint8)

    depth_resized = cv2.resize(depth_img, (img.shape[1], img.shape[0]))
    cv2.imwrite("depth.png", depth_resized)
    print("[âœ“] Depth map saved as depth.png")
    return "depth.png"

def simulate_multiview_mesh(rgb_path, depth_path, output_path="fused_mesh.obj", num_views=12):
    print(f"[ðŸ”„] Generating mesh using {num_views} simulated views...")
    rgb = cv2.imread(rgb_path)
    depth = cv2.imread(depth_path, cv2.IMREAD_GRAYSCALE)

    h, w = depth.shape
    fx = fy = 500.0
    cx, cy = w // 2, h // 2

    color_raw = o3d.geometry.Image(rgb)
    depth_raw = o3d.geometry.Image(depth)

    rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(
        color_raw, depth_raw, depth_scale=1.0, depth_trunc=255.0, convert_rgb_to_intensity=False
    )

    intrinsic = o3d.camera.PinholeCameraIntrinsic(w, h, fx, fy, cx, cy)
    base_pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image, intrinsic)

    base_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=5.0, max_nn=30))
    all_pcds = [base_pcd]

    for i in range(1, num_views):
        angle = (2 * np.pi / num_views) * i
        R = base_pcd.get_rotation_matrix_from_axis_angle([0, angle, 0])
        pcd_rotated = base_pcd.rotate(R, center=(0, 0, 0))
        all_pcds.append(pcd_rotated)

    print(f"[ðŸŒ€] Simulated {num_views} views.")
    merged_pcd = all_pcds[0]
    for pcd in all_pcds[1:]:
        merged_pcd += pcd

    print("[ðŸ§ ] Merged all simulated views. Running surface reconstruction...")
    merged_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=5.0, max_nn=30))
    mesh, _ = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(merged_pcd, depth=9)
    mesh.compute_vertex_normals()

    o3d.io.write_triangle_mesh(output_path, mesh)
    print(f"[âœ“] Final multiview mesh saved as {output_path}")

prompt = "a medieval fantasy helmet with horns"
rgb = generate_image(prompt)
depth = generate_depth(rgb)
simulate_multiview_mesh(rgb, depth, output_path="fused_mesh.obj", num_views=8)

display(Image.open("generated.png"))
display(Image.open("depth.png"))

def show_mesh(filepath="fused_mesh.obj"):
    mesh = trimesh.load(filepath)
    if isinstance(mesh, trimesh.Scene):
        mesh = mesh.dump(concatenate=True)

    geometry = BufferGeometry(
        attributes={
            "position": BufferAttribute(mesh.vertices.astype(np.float32), normalized=False),
            "index": BufferAttribute(mesh.faces.astype(np.uint32).flatten(), normalized=False)
        }
    )
    material = MeshStandardMaterial(color='orange', metalness=0.2, roughness=0.6)
    mesh_obj = Mesh(geometry=geometry, material=material)

    camera = PerspectiveCamera(position=[2, 2, 2], fov=75,
                               children=[DirectionalLight(color='white', position=[3, 5, 1], intensity=0.6)])
    scene = Scene(children=[mesh_obj, camera, AmbientLight(intensity=0.5)],
                  background="#aaaaaa")

    renderer = Renderer(camera=camera, scene=scene, controls=[OrbitControls(controlling=camera)],
                        width=640, height=480)
    display(renderer)


show_mesh("fused_mesh.obj")

